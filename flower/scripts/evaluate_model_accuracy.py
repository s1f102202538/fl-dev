#!/usr/bin/env python3
"""
IIDãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡ã‚’è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆCIFAR-10ã€MiniCNNã€è¨“ç·´ã‚ã‚Šï¼‰
    python scripts/evaluate_model_accuracy.py

    # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æŒ‡å®šã—ã¦è¨“ç·´
    python scripts/evaluate_model_accuracy.py --epochs 10 --lr 0.001

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™
    python scripts/evaluate_model_accuracy.py --train-samples 5000 --epochs 5

    # MNISTç”¨ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡
    python scripts/evaluate_model_accuracy.py --model mini-cnn-mnist --dataset ylecun/mnist --epochs 10

    # SimpleCNNãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡
    python scripts/evaluate_model_accuracy.py --model simple-cnn --dataset uoft-cs/cifar10 --epochs 15

    # MOONãƒ¢ãƒ‡ãƒ«ï¼ˆprojection headã‚ã‚Šï¼‰ã®è¨“ç·´ã¨è©•ä¾¡
    python scripts/evaluate_model_accuracy.py --model mini-cnn --is-moon --use-projection-head --epochs 10

    # è¨“ç·´ãªã—ï¼ˆåˆæœŸåŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã®ã¿ï¼‰
    python scripts/evaluate_model_accuracy.py --no-train

    # ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ï¼ˆcheckpointï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è©•ä¾¡ï¼ˆè¨“ç·´ãªã—ï¼‰
    python scripts/evaluate_model_accuracy.py --checkpoint path/to/model.pth --no-train
"""

import argparse
import sys
from pathlib import Path

import torch
from datasets import load_dataset
from torch.utils.data import DataLoader

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# flake8: noqa: E402
from fed.data.data_transform_manager import DataTransformManager
from fed.data.transformed_dataset import TransformedDataset
from fed.task.cnn_task import CNNTask
from fed.util.create_model import create_model


def parse_args():
  """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹"""
  parser = argparse.ArgumentParser(
    description="IIDãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡ã‚’è¡Œã†",
    formatter_class=argparse.RawDescriptionHelpFormatter,
    epilog=__doc__,
  )

  # ãƒ¢ãƒ‡ãƒ«è¨­å®š
  parser.add_argument(
    "--model",
    type=str,
    default="mini-cnn",
    choices=["mini-cnn", "mini-cnn-mnist", "simple-cnn", "simple-cnn-mnist"],
    help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ç¨®é¡",
  )
  parser.add_argument(
    "--n-classes",
    type=int,
    default=10,
    help="åˆ†é¡ã‚¯ãƒ©ã‚¹æ•°",
  )
  parser.add_argument(
    "--is-moon",
    action="store_true",
    help="MOONãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆprojection headã®æœ‰ç„¡ã¯--use-projection-headã§åˆ¶å¾¡ï¼‰",
  )
  parser.add_argument(
    "--use-projection-head",
    action="store_true",
    default=False,
    help="MOONãƒ¢ãƒ‡ãƒ«ã§projection headã‚’ä½¿ç”¨ï¼ˆ--is-moonã¨çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ï¼‰",
  )
  parser.add_argument(
    "--no-use-projection-head",
    action="store_true",
    help="MOONãƒ¢ãƒ‡ãƒ«ã§projection headã‚’ä½¿ç”¨ã—ãªã„ï¼ˆæ˜ç¤ºçš„æŒ‡å®šï¼‰",
  )
  parser.add_argument(
    "--out-dim",
    type=int,
    default=256,
    help="MOONãƒ¢ãƒ‡ãƒ«ã®projection headå‡ºåŠ›æ¬¡å…ƒ",
  )

  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
  parser.add_argument(
    "--dataset",
    type=str,
    default="uoft-cs/cifar10",
    help="ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆHugging Face Hubå½¢å¼ï¼‰",
  )
  parser.add_argument(
    "--batch-size",
    type=int,
    default=64,
    help="ãƒãƒƒãƒã‚µã‚¤ã‚º",
  )
  parser.add_argument(
    "--train-samples",
    type=int,
    default=None,
    help="è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆNoneã®å ´åˆã¯å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰",
  )
  parser.add_argument(
    "--test-samples",
    type=int,
    default=None,
    help="ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆNoneã®å ´åˆã¯å…¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰",
  )

  # è¨“ç·´è¨­å®š
  parser.add_argument(
    "--epochs",
    type=int,
    default=10,
    help="è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•°",
  )
  parser.add_argument(
    "--lr",
    type=float,
    default=0.01,
    help="å­¦ç¿’ç‡",
  )
  parser.add_argument(
    "--no-train",
    action="store_true",
    help="è¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆåˆæœŸåŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯checkpointã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã®ã¿ï¼‰",
  )

  # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰è¨­å®š
  parser.add_argument(
    "--checkpoint",
    type=str,
    default=None,
    help="ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹",
  )

  # ãƒ¢ãƒ‡ãƒ«ä¿å­˜è¨­å®š
  parser.add_argument(
    "--save-model",
    type=str,
    default=None,
    help="è¨“ç·´å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‘ã‚¹",
  )

  # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
  parser.add_argument(
    "--device",
    type=str,
    default="auto",
    choices=["auto", "cuda", "cpu"],
    help="ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ï¼ˆautoã¯è‡ªå‹•æ¤œå‡ºï¼‰",
  )

  # å‡ºåŠ›è¨­å®š
  parser.add_argument(
    "--verbose",
    action="store_true",
    help="è©³ç´°ãªãƒ­ã‚°ã‚’è¡¨ç¤º",
  )

  return parser.parse_args()


def get_device(device_arg: str) -> torch.device:
  """ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—"""
  if device_arg == "auto":
    return torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  return torch.device(device_arg)


def load_iid_train_data(dataset_name: str, batch_size: int, train_samples: int = None) -> DataLoader:
  """IIDè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰

  Args:
      dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåï¼ˆHugging Face Hubå½¢å¼ï¼‰
      batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
      train_samples: ä½¿ç”¨ã™ã‚‹è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆNoneã®å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ï¼‰

  Returns:
      è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®DataLoader
  """
  print(f"ğŸ“¦ Loading train dataset: {dataset_name}")

  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
  if train_samples is not None:
    split_str = f"train[:{train_samples}]"
    print(f"   Using {train_samples} train samples")
  else:
    split_str = "train"
    print("   Using all available train samples")

  train_dataset = load_dataset(dataset_name, split=split_str)
  print(f"   Loaded {len(train_dataset)} samples")

  # ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã®æº–å‚™
  from fed.data.data_loader_config import DataLoaderConfig

  config = DataLoaderConfig(dataset_name=dataset_name)
  transform_manager = DataTransformManager(config)

  # PyTorch Datasetãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ä½œæˆ
  train_dataset_wrapped = TransformedDataset(train_dataset, transform=transform_manager.train_transforms)

  # DataLoaderã‚’ä½œæˆï¼ˆIIDãªã®ã§ shuffle=Trueï¼‰
  train_loader = DataLoader(
    train_dataset_wrapped,
    batch_size=batch_size,
    shuffle=True,  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¯ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    drop_last=True,  # æœ€å¾Œã®ä¸å®Œå…¨ãªãƒãƒƒãƒã¯å‰Šé™¤
  )

  return train_loader


def load_iid_test_data(dataset_name: str, batch_size: int, test_samples: int = None) -> DataLoader:
  """IIDãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰

  Args:
      dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåï¼ˆHugging Face Hubå½¢å¼ï¼‰
      batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
      test_samples: ä½¿ç”¨ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆNoneã®å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ï¼‰

  Returns:
      ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®DataLoader
  """
  print(f"ğŸ“¦ Loading test dataset: {dataset_name}")

  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
  if test_samples is not None:
    split_str = f"test[:{test_samples}]"
    print(f"   Using {test_samples} test samples")
  else:
    split_str = "test"
    print("   Using all available test samples")

  test_dataset = load_dataset(dataset_name, split=split_str)
  print(f"   Loaded {len(test_dataset)} samples")

  # ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã®æº–å‚™
  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‹ã‚‰è¨­å®šã‚’æ¨æ¸¬
  from fed.data.data_loader_config import DataLoaderConfig

  config = DataLoaderConfig(dataset_name=dataset_name)
  transform_manager = DataTransformManager(config)

  # PyTorch Datasetãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ä½œæˆ
  test_dataset_wrapped = TransformedDataset(test_dataset, transform=transform_manager.eval_transforms)

  # DataLoaderã‚’ä½œæˆï¼ˆIIDãªã®ã§ shuffle=Falseï¼‰
  test_loader = DataLoader(
    test_dataset_wrapped,
    batch_size=batch_size,
    shuffle=False,  # IIDãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãªã®ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ä¸è¦
    drop_last=False,  # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚drop_lastã¯False
  )

  return test_loader


def evaluate_model(model, test_loader, device, verbose=False):
  """ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡

  Args:
      model: è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
      test_loader: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
      device: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹
      verbose: è©³ç´°ãƒ­ã‚°ã®è¡¨ç¤º

  Returns:
      (loss, accuracy)ã®ã‚¿ãƒ—ãƒ«
  """
  print(f"\nğŸ” Evaluating model on {device}")
  print(f"   Total batches: {len(test_loader)}")

  model.to(device)
  loss, accuracy = CNNTask.test(model, test_loader, device)

  return loss, accuracy


def train_model(model, train_loader, epochs, lr, device, verbose=False):
  """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´

  Args:
      model: è¨“ç·´ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
      train_loader: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
      epochs: ã‚¨ãƒãƒƒã‚¯æ•°
      lr: å­¦ç¿’ç‡
      device: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹
      verbose: è©³ç´°ãƒ­ã‚°ã®è¡¨ç¤º

  Returns:
      æœ€çµ‚ã‚¨ãƒãƒƒã‚¯ã®å¹³å‡è¨“ç·´æå¤±
  """
  print(f"\nğŸ‹ï¸  Training model on {device}")
  print(f"   Epochs: {epochs}")
  print(f"   Learning rate: {lr}")
  print(f"   Batches per epoch: {len(train_loader)}")

  model.to(device)
  final_loss = CNNTask.train(
    net=model,
    train_loader=train_loader,
    epochs=epochs,
    lr=lr,
    device=device,
  )

  print(f"   âœ… Training completed. Final loss: {final_loss:.6f}")

  return final_loss


def main():
  """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
  args = parse_args()

  print("=" * 80)
  print("ğŸ¯ Model Training and Evaluation with IID Data")
  print("=" * 80)

  # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
  device = get_device(args.device)
  print("\nâš™ï¸  Configuration:")
  print(f"   Model: {args.model}")
  print(f"   Dataset: {args.dataset}")
  print(f"   Device: {device}")
  print(f"   Batch size: {args.batch_size}")
  print(f"   Classes: {args.n_classes}")

  if not args.no_train:
    print("   Training: YES")
    print(f"   Epochs: {args.epochs}")
    print(f"   Learning rate: {args.lr}")
  else:
    print("   Training: NO (evaluation only)")

  # MOONãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
  use_projection_head = args.use_projection_head
  if args.no_use_projection_head:
    use_projection_head = False

  if args.is_moon:
    print(f"   MOON model: projection_head={'ON' if use_projection_head else 'OFF'}, out_dim={args.out_dim}")

  # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¾ãŸã¯ãƒ­ãƒ¼ãƒ‰
  print(f"\nğŸ—ï¸  Creating model: {args.model}")
  model = create_model(
    model_name=args.model,
    is_moon=args.is_moon,
    out_dim=args.out_dim,
    n_classes=args.n_classes,
    use_projection_head=use_projection_head,
  )

  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
  if args.checkpoint:
    print(f"ğŸ“‚ Loading checkpoint: {args.checkpoint}")
    checkpoint_path = Path(args.checkpoint)
    if not checkpoint_path.exists():
      print(f"âŒ Error: Checkpoint file not found: {args.checkpoint}")
      sys.exit(1)

    state_dict = torch.load(args.checkpoint, map_location=device)
    model.load_state_dict(state_dict)
    print("   âœ… Checkpoint loaded successfully")
  else:
    print("   Using randomly initialized weights")

  # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
  total_params = sum(p.numel() for p in model.parameters())
  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
  print(f"   Total parameters: {total_params:,}")
  print(f"   Trainable parameters: {trainable_params:,}")

  # è¨“ç·´ã®å®Ÿè¡Œï¼ˆ--no-trainãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
  if not args.no_train:
    train_loader = load_iid_train_data(
      dataset_name=args.dataset,
      batch_size=args.batch_size,
      train_samples=args.train_samples,
    )

    train_loss = train_model(
      model=model,
      train_loader=train_loader,
      epochs=args.epochs,
      lr=args.lr,
      device=device,
      verbose=args.verbose,
    )

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    if args.save_model:
      save_path = Path(args.save_model)
      save_path.parent.mkdir(parents=True, exist_ok=True)
      torch.save(model.state_dict(), save_path)
      print(f"\nğŸ’¾ Model saved to: {save_path}")

  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰
  test_loader = load_iid_test_data(
    dataset_name=args.dataset,
    batch_size=args.batch_size,
    test_samples=args.test_samples,
  )

  # ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
  loss, accuracy = evaluate_model(
    model=model,
    test_loader=test_loader,
    device=device,
    verbose=args.verbose,
  )

  # çµæœã®è¡¨ç¤º
  print("\n" + "=" * 80)
  print("ğŸ“Š Evaluation Results")
  print("=" * 80)
  print(f"   Test Loss: {loss:.6f}")
  print(f"   Test Accuracy: {accuracy:.2f}%")
  print("=" * 80)

  # æœŸå¾…ã•ã‚Œã‚‹ç²¾åº¦ç¯„å›²ã®è¡¨ç¤ºï¼ˆå‚è€ƒæƒ…å ±ï¼‰
  print("\nğŸ“ Reference Information:")
  if args.checkpoint is None and args.no_train:
    print("   âš ï¸  Model is randomly initialized (not trained)")
    print(f"   Expected accuracy for random guessing: ~{100.0 / args.n_classes:.2f}%")
  elif not args.no_train:
    print("   âœ… Model was trained in this session")
    print(f"   Training epochs: {args.epochs}")
    print(f"   Final training loss: {train_loss:.6f}")
  else:
    print("   âœ… Model loaded from checkpoint")
    print("   Expected accuracy depends on training quality")

  if "cifar10" in args.dataset.lower():
    print("\n   CIFAR-10 Baseline Accuracies:")
    print("   - Random: ~10%")
    print("   - Simple CNN (well-trained): ~70-75%")
    print("   - ResNet (well-trained): ~90-95%")
  elif "mnist" in args.dataset.lower():
    print("\n   MNIST Baseline Accuracies:")
    print("   - Random: ~10%")
    print("   - Simple CNN (well-trained): ~98-99%")

  print("\nâœ… Evaluation completed successfully!")


if __name__ == "__main__":
  main()
