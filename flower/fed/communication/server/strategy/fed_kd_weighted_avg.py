import json
import os
from logging import WARNING
from typing import Callable, Dict, List, Optional, Tuple, Union, override

import torch
import torch.nn.functional as F
import wandb
from fed.util.communication_cost import calculate_data_size_mb
from fed.util.model_util import base64_to_batch_list, batch_list_to_base64, create_run_dir
from flwr.common import EvaluateIns, EvaluateRes, FitIns, FitRes, MetricsAggregationFn, Parameters, Scalar
from flwr.common.logger import log
from flwr.common.typing import UserConfig
from flwr.server.client_manager import ClientManager
from flwr.server.client_proxy import ClientProxy
from flwr.server.strategy import Strategy
from flwr.server.strategy.aggregate import weighted_loss_avg
from torch import Tensor


class FedKDWeightedAvg(Strategy):
  """Federated Knowledge Distillation with Weighted Average Logit Aggregation (FedKD-WA) strategy.

  This strategy performs knowledge distillation using weighted average aggregation of client logits.
  Key features:
  - Weighted average aggregation of client logits based on client performance
  - Quality-based filtering with batch-wise relative evaluation
  - Temperature-scaled knowledge distillation
  """

  def __init__(
    self,
    *,
    fraction_fit: float = 1.0,
    fraction_evaluate: float = 1.0,
    min_fit_clients: int = 5,
    min_evaluate_clients: int = 5,
    min_available_clients: int = 5,
    on_fit_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,
    on_evaluate_config_fn: Optional[Callable[[int], dict[str, Scalar]]] = None,
    accept_failures: bool = True,
    initial_parameters: Optional[Parameters] = None,
    fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,
    evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,
    run_config: UserConfig,
    use_wandb: bool = False,
    # Simplified logit filtering parameters
    logit_temperature: float = 3.0,  # æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    kd_temperature: float = 5.0,  # çŸ¥è­˜è’¸ç•™ç”¨æ¸©åº¦
    entropy_threshold: float = 0.01,  # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é–¾å€¤ï¼ˆæœ€å°å“è³ªä¿è¨¼ç”¨ï¼‰
    confidence_threshold: float = 0.08,  # ä¿¡é ¼åº¦é–¾å€¤ï¼ˆç¾å®Ÿçš„ãªå­¦ç¿’åˆæœŸå€¤ï¼‰
  ) -> None:
    self.fraction_fit = fraction_fit
    self.fraction_evaluate = fraction_evaluate
    self.min_fit_clients = min_fit_clients
    self.min_evaluate_clients = min_evaluate_clients
    self.min_available_clients = min_available_clients
    self.on_fit_config_fn = on_fit_config_fn
    self.on_evaluate_config_fn = on_evaluate_config_fn
    self.accept_failures = accept_failures
    self.initial_parameters = initial_parameters
    self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn
    self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn
    self.avg_logits: List[Tensor] = []

    # Simplified logit filtering parameters
    self.logit_temperature = logit_temperature
    self.entropy_threshold = entropy_threshold
    self.confidence_threshold = confidence_threshold
    self.kd_temperature = kd_temperature

    self.save_path, self.run_dir = create_run_dir(run_config)
    self.use_wandb = use_wandb

    # Initialise W&B if set
    if use_wandb:
      self._init_wandb_project()

    # A dictionary to store results as they come
    self.results: Dict = {}

    # é€šä¿¡ã‚³ã‚¹ãƒˆè¿½è·¡ç”¨ã®å¤‰æ•°
    self.communication_costs: Dict[str, List[float]] = {
      "server_to_client_logits_mb": [],  # ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¸ã®ãƒ­ã‚¸ãƒƒãƒˆé€ä¿¡ã‚³ã‚¹ãƒˆ
      "client_to_server_logits_mb": [],  # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã‚µãƒ¼ãƒã¸ã®ãƒ­ã‚¸ãƒƒãƒˆé€ä¿¡ã‚³ã‚¹ãƒˆ
      "total_round_mb": [],  # ãƒ©ã‚¦ãƒ³ãƒ‰ã”ã¨ã®ç·é€šä¿¡ã‚³ã‚¹ãƒˆ
    }

  def _init_wandb_project(self) -> None:
    """Initialize W&B project."""
    wandb_project_name = os.getenv("WANDB_PROJECT_NAME", "federated-learning-default")
    wandb.init(project=wandb_project_name, name=f"{str(self.run_dir)}-ServerApp-FedKD")

  def _store_results(self, tag: str, results_dict: Dict) -> None:
    """Store results in dictionary, then save as JSON."""
    # Update results dict
    if tag in self.results:
      self.results[tag].append(results_dict)
    else:
      self.results[tag] = [results_dict]

    # Save results to disk
    with open(f"{self.save_path}/results.json", "w", encoding="utf-8") as fp:
      json.dump(self.results, fp)

  def store_results_and_log(self, server_round: int, tag: str, results_dict: Dict) -> None:
    """A helper method that stores results and logs them to W&B if enabled."""
    # Store results
    self._store_results(
      tag=tag,
      results_dict={"round": server_round, **results_dict},
    )

    if self.use_wandb:
      # Log metrics to W&B
      wandb.log(results_dict, step=server_round)

  def _evaluate_logit_quality(self, logits: Tensor) -> Dict[str, float]:
    """ãƒ­ã‚¸ãƒƒãƒˆã®å“è³ªã‚’è©•ä¾¡ã™ã‚‹ï¼ˆNon-IIDç’°å¢ƒå¯¾å¿œç‰ˆï¼‰

    Args:
        logits: è©•ä¾¡å¯¾è±¡ã®ãƒ­ã‚¸ãƒƒãƒˆãƒ†ãƒ³ã‚½ãƒ«

    Returns:
        å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¾æ›¸
    """
    with torch.no_grad():
      # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
      logits_clipped = torch.clamp(logits, min=-20, max=20)

      # æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚ã‚Šã¨ãªã—ã®ç¢ºç‡ã‚’è¨ˆç®—
      probs_raw = F.softmax(logits_clipped, dim=1)
      probs_temp = F.softmax(logits_clipped / self.logit_temperature, dim=1)

      # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚epsè¿½åŠ 
      eps = 1e-8
      probs_raw = torch.clamp(probs_raw, min=eps, max=1.0 - eps)
      probs_temp = torch.clamp(probs_temp, min=eps, max=1.0 - eps)

      # åŸºæœ¬å“è³ªæŒ‡æ¨™
      entropy = -torch.sum(probs_raw * torch.log(probs_raw), dim=1).mean().item()
      max_prob = probs_raw.max(dim=1)[0].mean().item()
      logit_variance = logits_clipped.var(dim=1).mean().item()

      # Non-IIDç’°å¢ƒå‘ã‘ã®è¿½åŠ æŒ‡æ¨™
      # 1. ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã®å‡ä¸€æ€§ï¼ˆJensen-Shannon divergenceï¼‰
      uniform_dist = torch.ones_like(probs_raw[0]) / probs_raw.shape[1]
      js_divergence = (
        0.5
        * (F.kl_div(torch.log(probs_raw.mean(0)), uniform_dist, reduction="sum") + F.kl_div(torch.log(uniform_dist), probs_raw.mean(0), reduction="sum")).item()
      )

      # 2. äºˆæ¸¬ã®ä¸€è²«æ€§ï¼ˆbatchå†…ã®æ¨™æº–åå·®ï¼‰
      prediction_consistency = 1.0 - probs_raw.std(dim=0).mean().item()

      # 3. æ¸©åº¦èª¿æ•´å¾Œã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
      temp_entropy = -torch.sum(probs_temp * torch.log(probs_temp), dim=1).mean().item()

      # 4. ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨æœ€å¤§ç¢ºç‡ã®çµ„ã¿åˆã‚ã›ï¼‰
      confidence_score = max_prob * (1.0 / (1.0 + entropy))

      # 5. Non-IIDæŒ‡æ¨™ï¼ˆã‚¯ãƒ©ã‚¹åã‚Šæ¤œå‡ºï¼‰
      class_distribution = probs_raw.mean(0)
      non_iid_score = torch.std(class_distribution).item()

      return {
        "entropy": entropy,
        "max_prob": max_prob,
        "logit_variance": logit_variance,
        "temp_entropy": temp_entropy,
        "confidence_score": confidence_score,
        "js_divergence": js_divergence,
        "prediction_consistency": prediction_consistency,
        "non_iid_score": non_iid_score,
        "concentration": 1.0 / (entropy + eps),
      }

  def _should_filter_logit(self, quality: Dict[str, float]) -> Tuple[bool, str]:
    """ãƒ­ã‚¸ãƒƒãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã¹ãã‹ã‚’åˆ¤å®šï¼ˆç„¡åŠ¹åŒ–æ¸ˆã¿ï¼‰

    Args:
        quality: ãƒ­ã‚¸ãƒƒãƒˆã®å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹

    Returns:
        å¸¸ã«(False, "filtering_disabled")ã‚’è¿”ã™
    """
    # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ– - å…¨ã¦ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’å—ã‘å…¥ã‚Œã‚‹
    return False, "filtering_disabled"

  def _relative_quality_filter(self, batch_qualities: List[Dict[str, float]], target_keep_ratio: float = 0.7) -> List[bool]:
    """ç›¸å¯¾çš„å“è³ªã«åŸºã¥ããƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰

    Args:
        batch_qualities: ãƒãƒƒãƒå†…ã®å…¨ãƒ­ã‚¸ãƒƒãƒˆå“è³ªãƒªã‚¹ãƒˆ
        target_keep_ratio: ä¿æŒã—ãŸã„ãƒ­ã‚¸ãƒƒãƒˆã®å‰²åˆï¼ˆ0.0-1.0ï¼‰

    Returns:
        å„ãƒ­ã‚¸ãƒƒãƒˆã‚’ä¿æŒã™ã‚‹ã‹ã®boolean ãƒªã‚¹ãƒˆ
    """
    if not batch_qualities:
      return []

    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    quality_scores = [(i, q["confidence_score"]) for i, q in enumerate(batch_qualities)]
    quality_scores.sort(key=lambda x: x[1], reverse=True)

    # ä¿æŒã™ã‚‹æ•°ã‚’è¨ˆç®—
    num_to_keep = max(1, int(len(batch_qualities) * target_keep_ratio))

    # ä¸Šä½ã‚’ä¿æŒ
    keep_indices = set(idx for idx, _ in quality_scores[:num_to_keep])

    return [i in keep_indices for i in range(len(batch_qualities))]

  def _weighted_average_logit_aggregation(self, logits_batch_lists: List[List[Tensor]], client_weights: List[float]) -> List[Tensor]:
    """ãƒ‡ãƒ¼ã‚¿å¯¾å¿œé–¢ä¿‚ã‚’ä¿æŒã™ã‚‹åŠ é‡å¹³å‡ãƒ­ã‚¸ãƒƒãƒˆé›†ç´„

    é‡è¦: å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã®å¯¾å¿œé–¢ä¿‚ã‚’ç¶­æŒã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒã”ã¨ã«
    å“è³ªãƒ™ãƒ¼ã‚¹é¸æŠ + åŠ é‡å¹³å‡ã‚’è¡Œã„ã€å„ãƒãƒƒãƒã«å¯¾ã—ã¦å¿…ãš1ã¤ã®é›†ç´„ãƒ­ã‚¸ãƒƒãƒˆã‚’ç”Ÿæˆ

    é›†ç´„æ–¹å¼:
    1. ãƒãƒƒãƒã”ã¨ã®å“è³ªè©•ä¾¡ã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé¸æŠ
    2. é¸æŠã•ã‚ŒãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ­ã‚¸ãƒƒãƒˆã®åŠ é‡å¹³å‡è¨ˆç®—
    3. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé‡ã¿ã‚’è€ƒæ…®ã—ãŸæœ€çµ‚é›†ç´„

    Args:
        logits_batch_lists: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®ãƒ­ã‚¸ãƒƒãƒˆãƒãƒƒãƒãƒªã‚¹ãƒˆ
        client_weights: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®é‡ã¿ï¼ˆåŠ é‡å¹³å‡ç”¨ï¼‰

    Returns:
        å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã¨1:1å¯¾å¿œã™ã‚‹åŠ é‡å¹³å‡é›†ç´„æ¸ˆã¿ãƒ­ã‚¸ãƒƒãƒˆãƒªã‚¹ãƒˆ
    """
    if not logits_batch_lists:
      return []

    # å…¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§å…±é€šã™ã‚‹ãƒãƒƒãƒæ•°ã‚’æ±ºå®š
    min_batches = min(len(batches) for batches in logits_batch_lists)
    max_batches = max(len(batches) for batches in logits_batch_lists)

    if min_batches != max_batches:
      print(f"[FedKD-WA] Batch count mismatch across clients. Using {min_batches} batches (min: {min_batches}, max: {max_batches})")

    # é‡ã¿ã‚’æ­£è¦åŒ–
    total_weight = sum(client_weights)
    normalized_weights = [w / total_weight for w in client_weights]

    aggregated_batches = []
    batch_quality_metrics = []
    total_filtered = 0
    total_evaluated = 0

    # å„ãƒãƒƒãƒã‚’å€‹åˆ¥ã«å‡¦ç†ï¼ˆãƒ‡ãƒ¼ã‚¿å¯¾å¿œé–¢ä¿‚ä¿æŒï¼‰
    for batch_idx in range(min_batches):
      batch_logits_candidates = []  # (client_idx, logits, quality, weight)

      # Step 1: ã“ã®ãƒãƒƒãƒã®å…¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ­ã‚¸ãƒƒãƒˆã‚’è©•ä¾¡
      for client_idx, client_batches in enumerate(logits_batch_lists):
        if batch_idx < len(client_batches):
          logits = client_batches[batch_idx]
          quality = self._evaluate_logit_quality(logits)
          batch_logits_candidates.append((client_idx, logits, quality, normalized_weights[client_idx]))
          total_evaluated += 1

      if not batch_logits_candidates:
        # ã“ã®ãƒãƒƒãƒã«ã¯ãƒ­ã‚¸ãƒƒãƒˆãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print(f"[FedKD-WA] Warning: No logits for batch {batch_idx}")
        continue

      # Step 2: ã“ã®ãƒãƒƒãƒå†…ã§ã®ç›¸å¯¾å“è³ªè©•ä¾¡
      def composite_quality_score(quality_metrics):
        """è¤‡åˆå“è³ªã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰"""
        confidence = quality_metrics["confidence_score"]
        entropy_penalty = 1.0 / (1.0 + quality_metrics["entropy"])
        consistency = quality_metrics["prediction_consistency"]
        return 0.4 * confidence + 0.3 * entropy_penalty + 0.3 * consistency

      # å“è³ªé †ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼šé«˜å“è³ªãŒå…ˆé ­ï¼‰
      batch_logits_candidates.sort(key=lambda x: composite_quality_score(x[2]), reverse=True)

      # Step 3: å›ºå®šä¿æŒç‡ã«åŸºã¥ã„ã¦é¸æŠ
      num_candidates = len(batch_logits_candidates)
      keep_ratio = 0.7  # å›ºå®šä¿æŒç‡
      num_to_keep = max(1, int(num_candidates * keep_ratio))  # æœ€ä½1ã¤ã¯ä¿æŒ

      selected_candidates = batch_logits_candidates[:num_to_keep]
      filtered_count = num_candidates - num_to_keep
      total_filtered += filtered_count

      # Step 4: é¸æŠã•ã‚ŒãŸãƒ­ã‚¸ãƒƒãƒˆã§é‡ã¿ä»˜ãé›†ç´„
      if len(selected_candidates) == 1:
        # 1ã¤ã®ãƒ­ã‚¸ãƒƒãƒˆã®ã¿: ãã®ã¾ã¾ä½¿ç”¨
        _, logits, quality, _ = selected_candidates[0]
        aggregated_batches.append(logits)
        batch_quality_metrics.append(quality)
      else:
        # è¤‡æ•°ãƒ­ã‚¸ãƒƒãƒˆ: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé‡ã¿ãƒ™ãƒ¼ã‚¹ã®åŠ é‡å¹³å‡é›†ç´„
        batch_logits = [candidate[1] for candidate in selected_candidates]
        batch_weights = [candidate[3] for candidate in selected_candidates]

        # åŠ é‡å¹³å‡ç”¨ã®é‡ã¿æ­£è¦åŒ–
        total_batch_weight = sum(batch_weights)
        if total_batch_weight > 0:
          batch_weights = [w / total_batch_weight for w in batch_weights]

        # åŠ é‡å¹³å‡ã«ã‚ˆã‚‹é›†ç´„ï¼ˆWeighted Average Aggregationï¼‰
        stacked_logits = torch.stack(batch_logits)
        weight_tensor = torch.tensor(batch_weights, device=stacked_logits.device).view(-1, 1, 1)
        weighted_logits = (stacked_logits * weight_tensor).sum(dim=0)

        # é›†ç´„å“è³ªã‚’è©•ä¾¡
        aggregated_quality = self._evaluate_logit_quality(weighted_logits)
        aggregated_batches.append(weighted_logits)
        batch_quality_metrics.append(aggregated_quality)

      # ãƒãƒƒãƒå˜ä½ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆè©³ç´°ãƒ¢ãƒ¼ãƒ‰ï¼‰
      if batch_idx % 50 == 0 or filtered_count > 0:
        selected_clients = [candidate[0] for candidate in selected_candidates]
        filtered_clients = [candidate[0] for candidate in batch_logits_candidates[num_to_keep:]]
        if filtered_count > 0:
          print(f"[FedKD-WA] Batch {batch_idx}: kept clients {selected_clients}, filtered clients {filtered_clients}")

    # Step 5: å…¨ä½“çµ±è¨ˆã¨ãƒ­ã‚°å‡ºåŠ›
    if batch_quality_metrics and total_evaluated > 0:
      overall_quality = {
        "confidence_score": sum(q["confidence_score"] for q in batch_quality_metrics) / len(batch_quality_metrics),
        "entropy": sum(q["entropy"] for q in batch_quality_metrics) / len(batch_quality_metrics),
        "non_iid_score": sum(q.get("non_iid_score", 0) for q in batch_quality_metrics) / len(batch_quality_metrics),
        "prediction_consistency": sum(q["prediction_consistency"] for q in batch_quality_metrics) / len(batch_quality_metrics),
      }

      actual_filter_rate = total_filtered / total_evaluated * 100 if total_evaluated > 0 else 0.0

      print("[FedKD-WA] === Weighted Average Aggregation Report ===")
      print(f"  ğŸ¯ Filtering Rate: {actual_filter_rate:.1f}%")
      print(f"  ğŸ”¢ Filtered: {total_filtered}/{total_evaluated} client logits")
      print(f"  ğŸ“¦ Output Batches: {len(aggregated_batches)} (= input {min_batches})")
      print("  ğŸ”— Data Correspondence: MAINTAINED (1:1 mapping)")
      print(f"  ğŸ“‹ Avg Quality - Confidence: {overall_quality['confidence_score']:.4f}, Entropy: {overall_quality['entropy']:.4f}")
      print("  âš–ï¸  Aggregation Method: Weighted Average (client performance based)")
      print("  ============================================")

      # ãƒ‡ãƒ¼ã‚¿å¯¾å¿œé–¢ä¿‚ã®ç¢ºèª
      if len(aggregated_batches) == min_batches:
        print("  âœ… Perfect data correspondence maintained")
      else:
        print(f"  âš ï¸  Data correspondence issue: {len(aggregated_batches)} â‰  {min_batches}")

    return aggregated_batches

  @override
  def initialize_parameters(self, client_manager: ClientManager) -> Optional[Parameters]:
    """Initialize the (global) model parameters.

    Parameters
    ----------
    client_manager : ClientManager
        The client manager which holds all currently connected clients.

    Returns
    -------
    parameters : Optional[Parameters]
        If parameters are returned, then the server will treat these as the
        initial global model parameters.
    """

    # ã‚µãƒ¼ãƒã¯ãƒ¢ãƒ‡ãƒ«ã‚’æŒãŸãªã„ãŸã‚ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ None ã‚’è¿”ã™
    return None

  @override
  def configure_fit(self, server_round: int, parameters: Parameters, client_manager: ClientManager) -> List[Tuple[ClientProxy, FitIns]]:
    """Configure the next round of training with enhanced logits and communication cost measurement."""

    config = {}
    # ç¾åœ¨ã®ãƒ©ã‚¦ãƒ³ãƒ‰æƒ…å ±ã‚’è¿½åŠ 
    config["current_round"] = server_round

    # é›†ç´„ã•ã‚ŒãŸãƒ­ã‚¸ãƒƒãƒˆï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
    enhanced_logits = self.avg_logits

    # ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¸ã®é€šä¿¡ã‚³ã‚¹ãƒˆæ¸¬å®š
    server_to_client_mb = 0.0

    # å‰å›ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã§é›†ç´„ã•ã‚ŒãŸãƒ­ã‚¸ãƒƒãƒˆãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
    if enhanced_logits:
      logits_data = batch_list_to_base64(enhanced_logits)
      config["avg_logits"] = logits_data
      # ãƒ­ã‚¸ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã‚’æ¸¬å®š
      server_to_client_mb = calculate_data_size_mb(logits_data)
      # å›ºå®šæ¸©åº¦ã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€ä¿¡
      config["temperature"] = self.kd_temperature
      config["logit_temperature"] = self.logit_temperature
      print(
        f"[FedKD] Sending {len(enhanced_logits)} enhanced logit batches (KD temp: {self.kd_temperature:.3f}, logit temp: {self.logit_temperature:.3f}, size: {server_to_client_mb:.4f} MB)"
      )
    else:
      print("[FedKD] No logits available for this round")

    # æœ‰åŠ¹ã«ãªã£ã¦ã„ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—
    sample_size = int(self.fraction_fit * client_manager.num_available())
    clients = client_manager.sample(num_clients=sample_size, min_num_clients=self.min_fit_clients)

    # å®Ÿéš›ã®é€šä¿¡ã‚³ã‚¹ãƒˆã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ•°ã‚’è€ƒæ…®
    total_server_to_client_mb = server_to_client_mb * len(clients)
    self.communication_costs["server_to_client_logits_mb"].append(total_server_to_client_mb)

    print(f"[FedKD] Total server->client communication: {total_server_to_client_mb:.4f} MB ({len(clients)} clients)")

    fit_ins = FitIns(parameters, config)
    return [(client, fit_ins) for client in clients]

  @override
  def aggregate_fit(
    self,
    server_round: int,
    results: List[Tuple[ClientProxy, FitRes]],
    failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],
  ) -> Tuple[Optional[Parameters], dict[str, Scalar]]:
    """Aggregate training results with enhanced logit processing and communication cost measurement."""

    logits_batch_lists = []
    client_weights = []

    # é€šä¿¡ã‚³ã‚¹ãƒˆæ¸¬å®š
    total_logits_mb = 0.0

    for _, fit_res in results:
      # ãƒ­ã‚¸ãƒƒãƒˆã‚µã‚¤ã‚ºæ¸¬å®š
      if fit_res.metrics and "logits" in fit_res.metrics:
        logits_data = str(fit_res.metrics["logits"])
        logits_size_mb = calculate_data_size_mb(logits_data)
        total_logits_mb += logits_size_mb

      if "logits" in fit_res.metrics:
        # ãƒãƒƒãƒãƒªã‚¹ãƒˆå½¢å¼ã§ãƒ­ã‚¸ãƒƒãƒˆã‚’å–å¾—
        logits_batch_list = base64_to_batch_list(str(fit_res.metrics["logits"]))

        logits_batch_lists.append(logits_batch_list)
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®é‡ã¿ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ï¼‰
        client_weights.append(float(fit_res.num_examples))

    if logits_batch_lists and client_weights:
      print(f"[FedKD] Aggregating logits from {len(logits_batch_lists)} clients")

      # é‡ã¿ä»˜ããƒ­ã‚¸ãƒƒãƒˆé›†ç´„ã‚’å®Ÿè¡Œ
      self.avg_logits = self._weighted_average_logit_aggregation(logits_batch_lists, client_weights)

      print(f"[FedKD-WA] Successfully aggregated {len(self.avg_logits)} batches using weighted average")
    else:
      print("[FedKD] No valid logits received from clients")

    # é€šä¿¡ã‚³ã‚¹ãƒˆã‚’è¨˜éŒ²
    self.communication_costs["client_to_server_logits_mb"].append(total_logits_mb)

    # ãƒ©ã‚¦ãƒ³ãƒ‰ã®ç·é€šä¿¡ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—
    server_to_client_logits_mb = self.communication_costs["server_to_client_logits_mb"][-1] if self.communication_costs["server_to_client_logits_mb"] else 0.0
    total_round_mb = server_to_client_logits_mb + total_logits_mb
    self.communication_costs["total_round_mb"].append(total_round_mb)

    print(
      f"[FedKD] Round {server_round}: Server->Client: {server_to_client_logits_mb:.4f} MB, Client->Server logits: {total_logits_mb:.4f} MB, total: {total_round_mb:.4f} MB"
    )

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®é›†ç´„
    aggregated_metrics = {}
    if self.fit_metrics_aggregation_fn:
      fit_metrics = [(res.num_examples, res.metrics) for _, res in results]
      aggregated_metrics = self.fit_metrics_aggregation_fn(fit_metrics)

    # ç¾åœ¨ã®æ¸©åº¦ã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ 
    aggregated_metrics["current_kd_temperature"] = self.kd_temperature
    aggregated_metrics["current_logit_temperature"] = self.logit_temperature
    if self.avg_logits:
      aggregated_metrics["num_aggregated_batches"] = len(self.avg_logits)

    # é€šä¿¡ã‚³ã‚¹ãƒˆã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ 
    aggregated_metrics["comm_cost_server_to_client_mb"] = server_to_client_logits_mb
    aggregated_metrics["comm_cost_client_to_server_logits_mb"] = total_logits_mb
    aggregated_metrics["comm_cost_total_round_mb"] = total_round_mb
    aggregated_metrics["comm_cost_cumulative_mb"] = sum(self.communication_costs["total_round_mb"])

    # é€šä¿¡ã‚³ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’W&Bã«ãƒ­ã‚°
    communication_metrics = {
      "comm_cost_server_to_client_mb": server_to_client_logits_mb,
      "comm_cost_client_to_server_logits_mb": total_logits_mb,
      "comm_cost_total_round_mb": total_round_mb,
      "comm_cost_cumulative_mb": sum(self.communication_costs["total_round_mb"]),
      "current_kd_temperature": self.kd_temperature,
      "current_logit_temperature": self.logit_temperature,
    }

    # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯å‰Šé™¤æ¸ˆã¿ï¼ˆç°¡ç´ åŒ–ã®ãŸã‚ï¼‰

    if self.avg_logits:
      communication_metrics["num_aggregated_batches"] = len(self.avg_logits)

    self.store_results_and_log(server_round=server_round, tag="communication_costs", results_dict=communication_metrics)

    return None, aggregated_metrics

  @override
  def configure_evaluate(self, server_round: int, parameters: Parameters, client_manager: ClientManager) -> List[Tuple[ClientProxy, EvaluateIns]]:
    """Configure the next round of evaluation.

    Parameters
    ----------
    server_round : int
        The current round of federated learning.
    parameters : Parameters
        The current (global) model parameters.
    client_manager : ClientManager
        The client manager which holds all currently connected clients.

    Returns
    -------
    evaluate_configuration : List[Tuple[ClientProxy, EvaluateIns]]
        A list of tuples. Each tuple in the list identifies a `ClientProxy` and the
        `EvaluateIns` for this particular `ClientProxy`. If a particular
        `ClientProxy` is not included in this list, it means that this
        `ClientProxy` will not participate in the next round of federated
        evaluation.
    """

    # è©•ä¾¡ç”¨ã®è¨­å®šã‚’ä½œæˆ
    config = {}

    # ç¾åœ¨ã®ãƒ©ã‚¦ãƒ³ãƒ‰æƒ…å ±ã‚’è¿½åŠ 
    config["current_round"] = server_round

    # å‰å›ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã§é›†ç´„ã•ã‚ŒãŸãƒ­ã‚¸ãƒƒãƒˆãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
    if self.avg_logits:
      config["avg_logits"] = batch_list_to_base64(self.avg_logits)
    # åˆå›ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã¯ãƒ­ã‚¸ãƒƒãƒˆãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€avg_logitsã‚­ãƒ¼ã‚’å«ã‚ãªã„
    evaluate_ins = EvaluateIns(parameters, config)

    # è©•ä¾¡ã«å‚åŠ ã™ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    sample_size = int(self.fraction_evaluate * client_manager.num_available())
    clients = client_manager.sample(num_clients=sample_size, min_num_clients=self.min_evaluate_clients)

    # Return client/config pairs
    return [(client, evaluate_ins) for client in clients]

  @override
  def aggregate_evaluate(
    self,
    server_round: int,
    results: List[Tuple[ClientProxy, EvaluateRes]],
    failures: List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],
  ) -> Tuple[Optional[float], Dict[str, Scalar]]:
    """Aggregate evaluation results.

    Parameters
    ----------
    server_round : int
        The current round of federated learning.
    results : List[Tuple[ClientProxy, FitRes]]
        Successful updates from the
        previously selected and configured clients. Each pair of
        `(ClientProxy, FitRes` constitutes a successful update from one of the
        previously selected clients. Not that not all previously selected
        clients are necessarily included in this list: a client might drop out
        and not submit a result. For each client that did not submit an update,
        there should be an `Exception` in `failures`.
    failures : List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]]
        Exceptions that occurred while the server was waiting for client updates.

    Returns
    -------
    aggregation_result : Tuple[Optional[float], Dict[str, Scalar]]
        The aggregated evaluation result. Aggregation typically uses some variant
        of a weighted average.
    """

    if not results:
      return None, {}
    # Do not aggregate if there are failures and failures are not accepted
    if not self.accept_failures and failures:
      return None, {}

    # Aggregate loss
    loss_aggregated = weighted_loss_avg([(evaluate_res.num_examples, evaluate_res.loss) for _, evaluate_res in results])

    # Aggregate custom metrics if aggregation fn was provided
    metrics_aggregated = {}
    if self.evaluate_metrics_aggregation_fn:
      eval_metrics = [(res.num_examples, res.metrics) for _, res in results]
      metrics_aggregated = self.evaluate_metrics_aggregation_fn(eval_metrics)
    elif server_round == 1:  # Only log this warning once
      log(WARNING, "No evaluate_metrics_aggregation_fn provided")

    # ç²¾åº¦æƒ…å ±ã®ãƒ­ã‚°å‡ºåŠ›
    if "accuracy" in metrics_aggregated:
      accuracy = metrics_aggregated["accuracy"]
      print(f"[FedKD] Round {server_round} - Accuracy: {accuracy:.4f}, Loss: {loss_aggregated:.4f}")

    # Store and log FedKD evaluation results
    self.store_results_and_log(
      server_round=server_round,
      tag="federated_evaluate",
      results_dict={"federated_evaluate_loss": loss_aggregated, **metrics_aggregated},
    )

    return loss_aggregated, metrics_aggregated

  @override
  def evaluate(self, server_round: int, parameters: Parameters) -> Optional[Tuple[float, Dict[str, Scalar]]]:
    """Evaluate the current model parameters.

    FedKD uses logit-based knowledge distillation instead of parameter aggregation.
    Server-side centralized evaluation is not applicable for this strategy.

    Parameters
    ----------
    server_round : int
        The current round of federated learning.
    parameters: Parameters
        The current (global) model parameters (unused in FedKD).

    Returns
    -------
    evaluation_result : Optional[Tuple[float, Dict[str, Scalar]]]
        Always returns None as FedKD does not perform centralized evaluation.
    """
    # FedKDã¯ãƒ­ã‚¸ãƒƒãƒˆãƒ™ãƒ¼ã‚¹ã®çŸ¥è­˜è’¸ç•™ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€
    # ã‚µãƒ¼ãƒãƒ¼å´ã§ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è©•ä¾¡ã¯è¡Œã‚ãªã„
    return None
